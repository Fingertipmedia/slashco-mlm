= Slashco: MLM and Sales Compensation Modeling
Kevin Van Gundy <kevin@neotechnology.com>
:neo4j-version: 2.2
:author: Kevin Van Gundy
:twitter: @kevinvangundy

[cols="1*", border="3"]
|===
a|image::http://i.imgur.com/TgNK2aN.png[align="center"]
|===

== Intro

Compensation planning and calculation in complex sales organizations can be incredibly taxing on traditional databases. Sufficiently large organizations often will end up batching these processes over night or into a weekly job Let’s take a look at Slashco, a multi-level marketing [s]organization[/s] “guild" in the World of Warcraft.

In this example, we have sales-mages who will both sell products directly as well as recruit people to also sell products on behalf of Slashco. I created a sample compensation model based on popular multi-level marketing model. It comprises of four income streams:

- Direct Sales: For each item a team-member sells directly, they earn a commission.
- Downstream Royalty: A percentage earned of the retail price of items sold by people “within their downstream,” meaning those that they have recruited to work for Slashco and by extension those who their recruits have recruited
- Wholesale Profit: A percentage earned on the goods sold to those “within their downstream” (these goods will then be re-sold to consumers
- Global Sales Royalty: A percentage of all sales made by those within Slashco. Revenue sharing.

Here’s an example of how a typical MLM organization might compensate its associates:

[cols="1*", border="3"]
|===
a|image::http://i.imgur.com/ODDXeKb.png[align="center"]
|===

We see that depending on what level you’re at, how you’re compensated both for your direct sales but also for your passive income streams can vary greatly. As such, calculating compensation can be hairy to say the least.

If we were to migrate our data from a legacy sales compensation planner, it would be best to push all of it out into a series of .csv files. In the database world, comma separated value files are often the lowest common denominator—thus for 90% of the projects I work on, I start there.  Let’s pretend we have pushed our old MySQL data into a series of 3 .csv files:

- https://github.com/kvangundy/Slashco/blob/master/employees.csv[employees.csv]

- https://github.com/kvangundy/Slashco/blob/master/item.csv[item.csv]

- https://github.com/kvangundy/Slashco/blob/master/transactions.csv[transactions.csv]

These are idealized dumps where I’ve synthesized what would normally be about a dozen csv files into 3 simply to show an example of how the data looks before and after Neo4j’s ETL process (LOAD CSV). When inspected by Excel (you could look at them with any other text editor as well), we’ll see a series of headers and rows with values that correspond to those headers. For example in the image below, we can see that a transaction with an ID of 1000000 has a value of 30 under the header of “salesRepID”, 22 under the header of “period”, 7780 under the header of “item1”, etc. etc. We’ll need to tell Neo4j how to interpret this information into a series of nodes and relationships instead of square “row and column” data.

[cols="1*", border="3"]
|===
a|image::http://i.imgur.com/cSnwjpg.png[align="center"]
|===

Before we actually load any data into Neo4j, it’s important to know what questions will be important to our business. In this case, we’re modeling a sales compensation tool. Our top queries might be something like:

 1. Commission due to each rep, by period in accord with compensation rules
 2. Commission due to each rep, annually in accord with compensation rules
 3. Sales Leaderboard
        - Top Sales Rep by Total Sales
        - Top Sales Rep by Largest Deal
 4. Global reporting
         - Sales by period
         - Top selling items
 5. Recommendations
         - What items are most frequently sold together?

When we know these questions, we can then build a model that makes it easy to answer those questions quickly and efficiently. As a general rule, if a piece of data is important to your use-case “make it physical” meaning, if you’re going to be traversing or filtering based on a specific bit of information make it a node or relationship. For example, Because we know that “deals” are important to our Sales Leaderboard, we’re going to make the idea of a deal (transaction) a specific type of node. We’ll then add information about that transaction, like when it happened, who owns it, and what items the deal contained. However, as a “last step” we’ll be doing some basic math operations on price, which is specific to that item— so we’ll keep that as a property.

== Slashco Sales Comp. Application Data Model:

[cols="1*", border="3"]
|===
a|image::http://i.imgur.com/2y4MfIX.png[align="center"]
|===

//hide
//setup
[source, cypher]
----
WITH range(1,52) as periods
FOREACH (period IN periods |
  MERGE (p:Period {period:period}));
----

//hide
//setup
[source, cypher]
----
//
MATCH (p:Period)
WITH p
ORDER BY p.period
WITH COLLECT(p) as periods
FOREACH (i in RANGE(0,length(periods)-2) |
  FOREACH(p1 in [periods[i]] |
      FOREACH(p2 in [periods[i+1]] |
          CREATE UNIQUE (p1)-[:NEXT]->(p2))));
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/itemSMALL.csv" as line
WITH line, toFLOAT(line.price) as price, toINT(line.item) as itemID, toFLOAT(line.kicker) as kick, toFLOAT(line.wprice) as wholesale
CREATE (:Item {itemID:itemID, name:line.name, price:price, kicker:kick, wholesalePrice:wholesale});
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/employees.csv" as line
WITH line, toINT(line.employeeID) as empID
CREATE (:Person {employeeID:empID, name:line.name});
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/employees.csv" as line
WITH line, toINT(line.employeeID) as empID, toINT(line.reportsTo) as reportsToID
MATCH (sub:Person {employeeID:empID}), (boss:Person {employeeID:reportsToID})
MERGE (sub)-[:REPORTS_TO]->(boss);
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/transactionsSMALL.csv" as line
WITH line, toINT(line.transactionID) as transID
CREATE (:Transaction {transactionID:transID});
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/transactionsSMALL.csv" as line
WITH line, toINT(line.transactionID) as transID, toINT(line.period) as period
MATCH (t:Transaction {transactionID:transID}), (p:Period {period:period})
CREATE (t)-[:OCCURRED_IN]->(p);
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/transactionsSMALL.csv" as line
WITH line,
toINT(line.transactionID) as transID,
toINT(line.item1) as itemID1,
toINT(line.item2) as itemID2,
toINT(line.item3) as itemID3
MATCH
(tx:Transaction {transactionID:transID}),
(i1:Item {itemID:itemID1}),
(i2:Item {itemID:itemID2}),
(i3:Item {itemID:itemID3})
CREATE
(tx)-[:CONTAINS]->(i1),
(tx)-[:CONTAINS]->(i2),
(tx)-[:CONTAINS]->(i3);
----

//hide
//setup
[source, cypher]
----
LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/kvangundy/Slashco/master/transactionsSMALL.csv" as line
WITH line,
toINT(line.transactionID) as transID,
toINT(line.salesRepID) as repID
MATCH (rep:Person {employeeID:repID}),
(tx:Transaction {transactionID:transID})
CREATE
(rep)-[:SOLD]->(tx);
----

//hide
//setup
[source, cypher]
----
MATCH (target:Person)<-[r:REPORTS_TO*..]-(e)
WITH target, count(e) as totalReports
SET target.reportsCount = totalReports
WITH target,
//setting the right "level" based on number of reports
CASE
WHEN target.reportsCount > 124
THEN 6
WHEN target.reportsCount < 124 and target.reportsCount >= 75
THEN 5
WHEN target.reportsCount < 75 and target.reportsCount >= 25
THEN 4
WHEN target.reportsCount < 25 and target.reportsCount >= 10
THEN 3
WHEN target.reportsCount < 10 and target.reportsCount >= 2
THEN 2
ELSE 1
END AS levels
SET target.level = levels;
----

The scripts I’m using to build this blog post are located in https://github.com/kvangundy/Slashco/[this git repo]

Now that we have a data model, let’s fire up Neo4j and pass in our import script. Found Here. Essentially what we’re doing is creating a few constraints and indexes, then telling Neo4j how to interpret our csv files into the above model.

[cols="1*", border="3"]
|===
a|image::http://zippy.gfycat.com/IllinformedJaggedAlbertosaurus.gif[align="center"]
|===

Now that we’ve loaded in all of our data, let’s open up our browser and start answering some of our top queries.

explode.gif
We’ll work backwards:

     5. Recommendations

    - What items are most frequently sold together?

[source, cypher]
----
//recommendation engine, what items are most frequently co-sold?
MATCH path = (item:Item)-[:CONTAINS]-(:Transaction)-[:CONTAINS]-(item2:Item)
WHERE id(item) > id(item2)
WITH item, item2, count(distinct path) as instances
ORDER BY instances DESC
LIMIT 3
RETURN item.name, item2.name, instances;
----
//table

     4. Global reporting

    - Sales by period
    - Top selling items

[source, cypher]
----
//total sales volume by period descending
MATCH (p:Period)-[:OCCURED_IN]-(t:Transaction)-[:CONTAINS]-(i:Item)
WITH sum(i.price) as sales, p
ORDER BY sales DESC
LIMIT 10
RETURN sales, p.period;
----
//table

[source, cypher]
----
MATCH (t:Transaction)-[:CONTAINS]-(i:Item)
WITH count(distinct(t)) as itemSales, i
ORDER BY itemSales DESC
LIMIT 5
RETURN i.name as name, itemSales as count;
----
//table

     3. Sales Leaderboard

    - Top Sales Rep by Total Sales Volume
    - Top Sales Rep by Largest Deal

[source, cypher]
----
//Who has sold the most volume?
MATCH (rep)-[:SOLD]-(txn)-[:CONTAINS]-(itm)
WITH rep, round(sum(itm.price)) as volume
ORDER BY volume DESC
LIMIT 5
RETURN rep.name as name, volume;
----
//table

[source, cypher]
----
//Who closed the largest deal?
MATCH (rep)-[:SOLD]-(txn)
WITH rep, txn
MATCH (txn)-[:CONTAINS]-(itm)
WITH rep, txn, round(sum(itm.price)) as dealSize
ORDER BY dealSize DESC
LIMIT 5
RETURN rep.name as name, txn.transactionID as transction, dealSize as `deal size`;
----
//table

     2. Commission due to each rep, annually in accord with compensation rules

Due to the complexity of the queries, I decided to run them with each level of rep separated out into its own query, however they all follow the basic form of the “what do I do with all this gold” query:

[source, cypher]
----
//level 6 comp
MATCH (transaction)-[:CONTAINS]-(item)
WITH sum(item.price*.05) as globalRoyalty
MATCH (big_boss:Person {level:6})<-[r:REPORTS_TO*..]-(downStreamers)-[:SOLD]-(transaction)-[:CONTAINS]-(item)
WITH sum(item.price*.1)+sum(item.wholesalePrice*.5) + globalRoyalty as downStreamGlobal6, big_boss
MATCH (boss)-[:SOLD]-(transaction)-[:CONTAINS]-(item)
WITH sum(item.price*.65) + downStreamGlobal6 as tc6, big_boss.name as n6
RETURN tc6, n6;
----
//table

[cols="1*", border="3"]
|===
a|image::http://zippy.gfycat.com/PleasedFalseFlatcoatretriever.gif[align="center"]
|===

     1. Commission due to each rep, by period in accord with compensation rules

This looks frighteningly similar to our last query, except we’ve added a short pattern `(transaction)-[:OCCURRED_IN]-(period {period:35})` which will filter out all transactions that occurred in periods that are not the 35th.

[source, cypher]
----
//level 6 comp
MATCH (transction)-[:CONTAINS]-(item),
(transaction)-[:OCCURED_IN]-(p:Period {period:35})
WITH sum(item.price*.05) as globalRoyalty
MATCH (big_boss:Person {level:6})<-[r:REPORTS_TO*..]-(downStreamers)-[:SOLD]-(transction)-[:CONTAINS]-(item),
(transaction)-[:OCCURED_IN]-(p:Period {period:35})
WITH sum(item.price*.1)+sum(item.wholesalePrice*.5) + globalRoyalty as downStreamGlobal6, big_boss
MATCH (boss)-[:SOLD]-(transction)-[:CONTAINS]-(item),
(transaction)-[:OCCURED_IN]-(p:Period {period:35})
WITH sum(item.price*.65) + downStreamGlobal6 as tc6, big_boss.name as n6
RETURN tc6, n6;
----
//table

[cols="1*", border="3"]
|===
a|image::http://zippy.gfycat.com/PolishedGreenCaterpillar.gif[align="center"]
|===

 --kvg
